{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (194183010.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install scikit-learn\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n",
    "pip install \"mne>=1.0\" matplotlib mne-qt-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: QtAgg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import mne_qt_browser\n",
    "\n",
    "#from matplotlib.colors import TwoSlopeNorm\n",
    "import math \n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.stats import permutation_cluster_1samp_test as pcluster_test\n",
    "from mne.time_frequency import tfr_multitaper, tfr_morlet\n",
    "from mne.time_frequency import morlet, fwhm\n",
    "import tables as tb\n",
    "from csv import writer \n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')  # Use the Qt5 backend\n",
    "\n",
    "%matplotlib\n",
    "\n",
    "class EEGDataProcessor:\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw = None\n",
    "        self.epochs = None \n",
    "        self.clean_epochs = None\n",
    "        self.ICA = None\n",
    "        \n",
    "    def load_epochs_from_fif(self, path_to_epochs_file):\n",
    "        \"\"\"Load preprocessed epochs from a .fif file.\"\"\"\n",
    "        # Load epochs from the .fif file\n",
    "        self.epochs = mne.read_epochs(path_to_epochs_file)\n",
    "        \n",
    "\n",
    "    def read_file(self, path):\n",
    "        r = mne.io.read_raw_bdf(path, preload=False)\n",
    "        \n",
    "        # Define signal info\n",
    "        n_time_samps = r.n_times\n",
    "        time_secs = r.times\n",
    "        ch_names = r.ch_names\n",
    "        n_chan = len(ch_names) \n",
    "        \n",
    "        \n",
    "        channels_to_keep = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\", \"A9\", \"A10\",\n",
    "                \"A11\", \"A12\", \"A13\", \"A14\", \"A15\", \"A16\", \"A17\", \"A18\", \"A19\", \"A20\",\n",
    "                \"A21\", \"A22\", \"A23\", \"A24\", \"A25\", \"A26\", \"A27\", \"A28\", \"A29\", \"A30\",\n",
    "                \"A31\", \"A32\", \"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B9\", \"B10\",\n",
    "                \"B11\", \"B12\", \"B13\", \"B14\", \"B15\", \"B16\", \"B17\", \"B18\", \"B19\", \"B20\",\n",
    "                \"B21\", \"B22\", \"B23\", \"B24\", \"B25\", \"B26\", \"B27\", \"B28\", \"B29\", \"B30\", \"B31\", \"B32\"]\n",
    "\n",
    "        channels_to_exclude_str = [ch for ch in r.ch_names if ch not in channels_to_keep + ['Status']]\n",
    "        \n",
    "        \n",
    "        raw = mne.io.read_raw_bdf(path, exclude = channels_to_exclude_str ,  preload=True)\n",
    "        \n",
    "        list_chan_name = list_chan_name = { 'A3': 'AF3',\n",
    "        'A7': 'F7', 'A6': 'F5','A5': 'F3', 'A4': 'F1',\n",
    "        'A8': 'FT7', 'A9': 'FC5','A10': 'FC3', 'A11': 'FC1',\n",
    "        'A15': 'T7', 'A14': 'C5','A13': 'C3', 'A12': 'C1',\n",
    "        'A16': 'TP7', 'A17': 'CP5','A18': 'CP3', 'A19': 'CP1', 'A32': 'CPz',\n",
    "        'A24': 'P9', 'A23': 'P7','A22': 'P5', 'A21': 'P3', 'A20': 'P1','A31': 'Pz',\n",
    "        'A25': 'PO7', 'A26': 'PO3','A30': 'POz', \n",
    "        'A27': 'O1',   'A29': 'Oz', \n",
    "        'A28': 'Iz', \n",
    "        'B1': 'Fpz','B2': 'Fp2',\n",
    "        'B5': 'AFz', 'B4': 'AF4','B3': 'AF8',\n",
    "        'B6': 'Fz', 'B7': 'F2','B8': 'F4', 'B9': 'F6','B10': 'F8',\n",
    "        'B15': 'FCz', 'B14': 'FC2','B13': 'FC4', 'B12': 'FC6','B11': 'FT8',\n",
    "        'B16': 'Cz', 'B17': 'C2','B18': 'C4', 'B19': 'C6','B20': 'T8',\n",
    "        'B24': 'CP2', 'B23': 'CP4','B22': 'CP6', 'B21': 'TP8',\n",
    "        'B25': 'P2', 'B26': 'P4','B27': 'P6', 'B28': 'P8','B29': 'P10',\n",
    "        'B31': 'PO4', 'B30': 'PO8',\n",
    "        'B32': 'O2'\n",
    "        }\n",
    "            \n",
    "        _ = raw.rename_channels(list_chan_name)\n",
    "        print(raw.info)\n",
    "        print(\"These are the channels and lenght of channnels,\", raw.ch_names, len(raw.ch_names))\n",
    "        \n",
    "        \n",
    "        self.raw = raw  # Store the raw data in the instance attribute\n",
    "        \n",
    "        return raw\n",
    "\n",
    "        \n",
    "    def highlow_pass(self):\n",
    "        if self.raw is None:\n",
    "            raise ValueError(\"Raw data is not loaded. Call read_file() first.\")\n",
    "\n",
    "        # Apply high-pass filter\n",
    "        self.raw.filter(l_freq=1, h_freq=None, fir_design='firwin', filter_length='auto',\n",
    "                        l_trans_bandwidth='auto', h_trans_bandwidth='auto', method='fir',\n",
    "                        phase='zero', fir_window='hamming', verbose=True)\n",
    "\n",
    "        # Apply low-pass filter if needed\n",
    "        self.raw.filter(l_freq=None, h_freq=40, fir_design='firwin', filter_length='auto',\n",
    "                        l_trans_bandwidth='auto', h_trans_bandwidth='auto', method='fir',\n",
    "                        phase='zero', fir_window='hamming', verbose=True)\n",
    "\n",
    "        return self.raw\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def rename_events(self):\n",
    "        \n",
    "        if self.raw is None:\n",
    "            raise ValueError(\"Raw data is not loaded. Call read_file() first.\")\n",
    "\n",
    "        min_duration = 1 / self.raw.info['sfreq']\n",
    "        # Find events and drop unnecessary\n",
    "        events_ID = mne.find_events(self.raw, stim_channel=\"Status\", initial_event=True, shortest_event=1, min_duration = min_duration)\n",
    "       # print('These are the events', events_ID)\n",
    "        events = mne.pick_events(events_ID, exclude=[3, 5, 40, 42, 62])\n",
    "        #print('These are the events after excluding:', events)\n",
    "\n",
    "\n",
    "        unsuccessful_event_index = None  # For sanity check\n",
    "\n",
    "        for i, event in enumerate(events):\n",
    "            if event[2] == 10:\n",
    "                next_event_index = i + 1\n",
    "                while next_event_index < len(events) and events[next_event_index][2] == 61:\n",
    "                    events[next_event_index][2] = 610  # Modify the trigger value\n",
    "                    next_event_index += 1\n",
    "            elif event[2] == 20:\n",
    "                next_event_index = i + 1\n",
    "                while next_event_index < len(events) and events[next_event_index][2] == 61:\n",
    "                    events[next_event_index][2] = 620  # Modify the trigger value\n",
    "                    next_event_index += 1\n",
    "            elif event[2] == 30:\n",
    "                next_event_index = i + 1\n",
    "                while next_event_index < len(events) and events[next_event_index][2] == 61:\n",
    "                    events[next_event_index][2] = 630  # Modify the trigger value\n",
    "                    next_event_index += 1\n",
    "            else:\n",
    "                unsuccessful_event_index = i\n",
    "\n",
    "        # Check if any events with trigger value 61 are still present\n",
    "        if any(event[2] == 61 for event in events):\n",
    "            print(\"Renaming was unsuccessful :(\")\n",
    "            if unsuccessful_event_index is not None:\n",
    "                print(\"Problematic event index:\", unsuccessful_event_index)\n",
    "                print(\"Problematic event details:\", events[unsuccessful_event_index])\n",
    "\n",
    "        # Define epoch parameters\n",
    "        tmin = -2\n",
    "        tmax = 3\n",
    "\n",
    "        \n",
    "        \n",
    "        # Rename events with numerical IDs to have corresponding event names\n",
    "        event_dict = {\"10\": 10, \"20\": 20, \"30\": 30, \"selftap\": 41, \"VP610\": 610, \"VP620\": 620, \"VP630\": 630}\n",
    "        #raw_resampled, events_resampled = self.raw.copy().resample(256, npad='auto', events=events)\n",
    "        #del raw_resampled\n",
    "        # Epoch signal\n",
    "        epochs = mne.Epochs(self.raw, events, event_id=event_dict, tmin=tmin, tmax=tmax, baseline=None)\n",
    "        \n",
    "        del self.raw                                # Delete raw data after processing\n",
    "        # Divide the epochs into smaller pieces\n",
    "        \n",
    "        epochs.drop_bad()\n",
    "        num_pieces = 4                           # split epochs into 4 pieces \n",
    "        piece_size = len(epochs) // num_pieces\n",
    "        resampled_epochs_pieces = []\n",
    "\n",
    "        for idx in range(num_pieces):\n",
    "            \n",
    "            start_idx = idx * piece_size\n",
    "            end_idx = start_idx + piece_size if idx < num_pieces - 1 else len(epochs)\n",
    "            epochs_piece = epochs[start_idx:end_idx]\n",
    "            epochs_piece.load_data()\n",
    "            epochs_rs_piece = epochs_piece.resample(sfreq=256)\n",
    "            resampled_epochs_pieces.append(epochs_rs_piece)\n",
    "\n",
    "        # Concatenate the resampled pieces\n",
    "        self.epochs = mne.concatenate_epochs(resampled_epochs_pieces)\n",
    "        \n",
    "    \n",
    "        #epochs.load_data()\n",
    "        #Convert to float32   \n",
    "        # Resample the epochs\n",
    "        #epochs_rs = epochs.resample(sfreq=256)\n",
    "        self.epochs.plot(picks= ['eeg'],n_channels=64,n_epochs= 25,block=True)\n",
    "        print(epochs.drop_log)\n",
    "        \n",
    "\n",
    "        return self.epochs\n",
    "\n",
    "\n",
    "\n",
    "    def plot_comp(self, epochs, picks, participant_id):\n",
    "        # Call plot_properties to generate the figures\n",
    "        figs = self.ICA.plot_properties(epochs, picks=picks, log_scale=False)\n",
    "        \n",
    "        # Save each figure separately\n",
    "        for i, fig in enumerate(figs):\n",
    "            fig.savefig(f'C:/Users/Mabel Ife/OneDrive - Danmarks Tekniske Universitet/Dokumenter/Master in AI/Final Thesis/Figures/ICA/ICA_component_{participant_id}_{i}.png')\n",
    "        \n",
    "    \n",
    "    def plotsignal(self, channels, pick_min, pick_max):\n",
    "        if self.epochs is None:\n",
    "            print(\"No epochs data available. Please load epochs data before plotting.\")\n",
    "            return\n",
    "        ch_names = self.epochs.ch_names\n",
    "        picks = ch_names[pick_min:pick_max]\n",
    "        self.epochs.plot(n_channels=channels, picks=picks, events=False, title='EEG Data for the Channels')\n",
    "        \n",
    "    \n",
    "    def drop_bad_epochs(self, bad_epoch_indices=None, p_id=None):\n",
    "        if self.epochs is None:\n",
    "            print(\"No epochs data available. Please run rename_events before dropping bad epochs.\")\n",
    "            return None\n",
    "\n",
    "        if bad_epoch_indices is not None:\n",
    "            # Drop bad epochs based on the provided indices\n",
    "            self.epochs.drop(bad_epoch_indices)\n",
    "            print(\"Bad epochs dropped based on provided indices.\")\n",
    "        else:\n",
    "            # Check if bad epochs were marked interactively during plotting\n",
    "            if not self.epochs.drop_log:\n",
    "                print(\"No bad epochs marked interactively. Nothing to drop.\")\n",
    "            else:\n",
    "                # Drop bad epochs marked interactively during plotting\n",
    "                self.epochs.drop()\n",
    "\n",
    "                print(\"Bad epochs dropped based on interactive marking.\")\n",
    "                \n",
    "                \n",
    "                \n",
    "        # After we drop bad trials we save the file in a dataframe\n",
    "        #direc = \"C:/Users/Mabel Ife/OneDrive - Danmarks Tekniske Universitet/Dokumenter/Thesis Code/Data/epochs/\"\n",
    "       \n",
    "        #self.epochs.save(direc + p_id + '-epo.fif', fmt='single', overwrite=True, verbose=None)\n",
    "        \n",
    "        return self.epochs \n",
    "        \n",
    "        #list = [p_id,self.epochs]\n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit_ica(self, p_id):\n",
    "        \n",
    "        \"\"\"\n",
    "        The objective is to remove, eye blinks, heartbeats and muscle artefact.Cleaning the data with ICA\n",
    "        \"\"\"\n",
    "        #epochs.load_data(dtype=np.float64)\n",
    "        # run fast ICA \n",
    "        ica = mne.preprocessing.ICA(\n",
    "            n_components=30, method=\"fastica\", max_iter=\"auto\", random_state=97)\n",
    "        \n",
    "       \n",
    "        \n",
    "        self.ICA = ica\n",
    "        \n",
    "        ica.fit(self.epochs) # fit on epochs\n",
    "        \n",
    "         \n",
    "        #direc = \"C:/Users/Mabel Ife/OneDrive - Danmarks Tekniske Universitet/Dokumenter/Thesis Code/Data/Ica_files/\"\n",
    "        #self.ICA.save(direc + p_id + '-ica.fif', overwrite=True)\n",
    "        # Visualise ICA components\n",
    "        \n",
    "        #self.epochs.set_montage(self.montage)\n",
    "\n",
    "        # plot the raw epoched signal and low and hihgpass\n",
    "        #mne.viz.set_browser_backend('qt') \n",
    "        ica.plot_sources(self.epochs) # plot the raw signal \n",
    "\n",
    "        # plot scalp field topographies for 30 components. Here we identify manual artifacts like eyeblinks etc.\n",
    "       \n",
    "        ica.plot_components() \n",
    "        \n",
    "        # We use MNE to identify muscle components\n",
    "        muscle_idx_auto, scores = ica.find_bads_muscle(self.epochs)\n",
    "        ica.plot_scores(scores, exclude=muscle_idx_auto)\n",
    "        print(\n",
    "        f\"Automatically found muscle artifact ICA components: {muscle_idx_auto}\"\n",
    "        )\n",
    "        \n",
    "        return self.ICA\n",
    "    \n",
    "        \n",
    "    def reject_artefact(self, comps):\n",
    "        self.ICA.exclude = comps \n",
    "        \n",
    "        reconst_epochs = self.epochs.copy()\n",
    "        self.ICA.apply(reconst_epochs)\n",
    "        \n",
    "        self.epochs = reconst_epochs\n",
    "        \n",
    "        return self.epochs\n",
    "             \n",
    "    def drop_bad_interp(self, channels_list):\n",
    "\n",
    "        if self.epochs is None:\n",
    "            print(\"No epochs data available. Please run rename_events before interpolating bad channels.\")\n",
    "            return None\n",
    "            \n",
    "        # ep_ds = self.epochs.copy() \n",
    "            \n",
    "        if not channels_list:\n",
    "            # No channels to drop\n",
    "            print(\"No channels to drop\")\n",
    "        else: \n",
    "            # Highlight bad channels\n",
    "            #ep_ds.info[\"bads\"].extend(channels_list)\n",
    "            self.epochs.info[\"bads\"].extend(channels_list)\n",
    "\n",
    "        #ep_ds.set_montage('standard_1020')\n",
    "\n",
    "        self.epochs.set_montage('standard_1020')\n",
    "        \n",
    "        # ep_interp = ep_ds.interpolate_bads(reset_bads=False, method='MNE', verbose=True)\n",
    "        self.epochs.interpolate_bads(reset_bads=False, method='spline', verbose=None)   \n",
    "        # average data \n",
    "        # use the average of all channels as reference\n",
    "        self.clean_epochs, ref_data = mne.set_eeg_reference(self.epochs, ref_channels=\"average\", projection=False)\n",
    "\n",
    "        return self.clean_epochs\n",
    "\n",
    "    def drop_bad_interp2(self, channels_list, separate_epochs=None):\n",
    "        if separate_epochs is None:\n",
    "            if self.epochs is None:\n",
    "                print(\"No epochs data available. Please run rename_events before interpolating bad channels.\")\n",
    "                return None\n",
    "            else:\n",
    "                epochs_data = self.epochs\n",
    "        else:\n",
    "            epochs_data = separate_epochs\n",
    "                \n",
    "        if not channels_list:\n",
    "            # No channels to drop\n",
    "            print(\"No channels to drop\")\n",
    "        else: \n",
    "            # Highlight bad channels\n",
    "            epochs_data.info[\"bads\"].extend(channels_list)\n",
    "\n",
    "        epochs_data.set_montage('standard_1020')\n",
    "        \n",
    "        epochs_data.interpolate_bads(reset_bads=True, method='spline', verbose=None)   \n",
    "        \n",
    "        # average data \n",
    "        # use the average of all channels as reference\n",
    "        self.clean_epochs, ref_data = mne.set_eeg_reference(epochs_data, ref_channels=\"average\", projection=False)\n",
    "\n",
    "        return self.clean_epochs\n",
    "    \n",
    "    def fit_imported_ica(self,ica, ep,del_comps=None):\n",
    "        \"\"\"\n",
    "        This function is for when you want to use saved ICA and have already have some components to drop or not.\n",
    "        \"\"\"\n",
    "        ica = ica \n",
    "        ica.fit(ep)\n",
    "        ica.plot_components() \n",
    "        ica.plot_sources(ep)\n",
    "        \n",
    "        if del_comps is not None:\n",
    "            ica.exclude = del_comps\n",
    "            ep.load_data()\n",
    "            reconst_ep = ica.apply(ep)\n",
    "            return reconst_ep\n",
    "        else: \n",
    "            \n",
    "            print(\"No ICA Components were chosen to be zeroed out. Run function again with ICA Components specified\")\n",
    "            return None\n",
    "\n",
    "\n",
    "processor = EEGDataProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_epochs(sub,data_dir,epochs):\n",
    "    # saves Epoch\n",
    "    epochs.save(data_dir + sub + '-epo.fif', fmt='single',overwrite=True)\n",
    "    \n",
    "    \n",
    "def save_ica(subject,directionary,ica):\n",
    "        \n",
    "        ica.save(directionary + subject+ '-ica.fif', overwrite=True)\n",
    "        \n",
    "def make_events(epochs):\n",
    "    \n",
    "    \n",
    "     # separate conditions\n",
    "    motor_source_event41 = epochs['selftap']\n",
    "    motor_source_event610 = epochs['VP610']\n",
    "    motor_source_event620 = epochs['VP620']\n",
    "    motor_source_event630 = epochs['VP630']\n",
    "        \n",
    "        # combine all conditions to one list \n",
    "    all_motor_components = [motor_source_event41, motor_source_event610, motor_source_event620, motor_source_event630]\n",
    "    \n",
    "    return all_motor_components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.set_browser_backend('qt') \n",
    "path = \"C:/Users/s184075/Thesis/epochs/subj_12-epo.fif\"\n",
    "processor.load_epochs_from_fif(path)\n",
    "\n",
    "montage = mne.channels.make_standard_montage(\"standard_1005\") \n",
    "ep = processor.epochs\n",
    "\n",
    "ep.set_montage(montage)\n",
    "del ep\n",
    "#processor.fit_ica(p_id=participant_id)\n",
    "processor.fit_ica(p_id='subj_12')\n",
    "processor.reject_artefact([0,3, 11, 15, 25, 28, 29])\n",
    "processor.drop_bad_interp(['TP8','T8','P1'])\n",
    "clean_ep = processor.clean_epochs\n",
    "\n",
    "processor.plotsignal(64,1,64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = \"C:/Users/s184075/Thesis/clean_epochs/\" \n",
    "save_epochs('subj_12',direc,clean_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using qt as 2D backend.\n",
      "Extracting EDF parameters from E:\\Libraries\\Mabel\\Thesis\\subjects\\subj_22_130223_task.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mabel Ife\\AppData\\Local\\Temp\\ipykernel_41808\\4278127755.py:40: RuntimeWarning: Number of records from the header does not match the file size (perhaps the recording was not stopped before exiting). Inferring from the file size.\n",
      "  r = mne.io.read_raw_bdf(path, preload=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from E:\\Libraries\\Mabel\\Thesis\\subjects\\subj_22_130223_task.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 5574655  =      0.000 ...  2722.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mabel Ife\\AppData\\Local\\Temp\\ipykernel_41808\\4278127755.py:59: RuntimeWarning: Number of records from the header does not match the file size (perhaps the recording was not stopped before exiting). Inferring from the file size.\n",
      "  raw = mne.io.read_raw_bdf(path, exclude = channels_to_exclude_str ,  preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: A1, A2, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, T7, ...\n",
      " chs: 64 EEG, 1 Stimulus\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 417.0 Hz\n",
      " meas_date: 2023-02-13 16:27:41 UTC\n",
      " nchan: 65\n",
      " projs: []\n",
      " sfreq: 2048.0 Hz\n",
      " subject_info: 1 item (dict)\n",
      ">\n",
      "These are the channels and lenght of channnels, ['A1', 'A2', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7', 'TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz', 'Pz', 'CPz', 'Fpz', 'Fp2', 'AF8', 'AF4', 'AFz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2', 'Status'] 65\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up high-pass filter at 1 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal highpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Filter length: 6759 samples (3.300 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 677 samples (0.331 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7030 events found on stim channel Status\n",
      "Event IDs: [    3     5    10    20    30    40    41    42    61    62 65536]\n",
      "Not setting metadata\n",
      "3367 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 3367 events and 10241 original time points ...\n",
      "5 bad epochs dropped\n",
      "Using data from preloaded Raw for 840 events and 10241 original time points ...\n",
      "Using data from preloaded Raw for 840 events and 10241 original time points ...\n"
     ]
    }
   ],
   "source": [
    "mne.viz.set_browser_backend('qt') \n",
    "import os \n",
    "#del rereferenced_epochs, ica\n",
    "# Specify 'bad_trials', only true if they have been identified\n",
    "\n",
    "#path = \"C:/Users/Mabel Ife/OneDrive - Danmarks Tekniske Universitet/Dokumenter/Thesis Code/Data/Subjects/subj_07_070223_task.bdf\"\n",
    "#path = \"C:/Users/s184075/Thesis/subjects/subj_21_130223_task.bdf\"\n",
    "path = \"E:/Libraries/Mabel/Thesis/subjects/subj_22_130223_task.bdf\"\n",
    "file_name = os.path.basename(path)  # Get the file name from the path\n",
    "participant_id = file_name[:7] # get P_Id \n",
    "# Create HDF5 file with st\n",
    "\n",
    "\n",
    "# Call method to read the file\n",
    "processor.read_file(path)\n",
    "\n",
    "# Call method for high and low paas \n",
    "processor.highlow_pass()\n",
    "\n",
    "# Call the method to rename events and create epochs\n",
    "processor.rename_events()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_epoch_indices = [17,109,264, 265, 266, 267, 347, 348, 349, 758, 855, 856, 857, 938, 939, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1209, 1210, 1211, 1245, 1246, 1247, 1248, 1261, 1262, 1263, 1264, 1265, 1689, 1732, 1758, 1798, 1808, 1809, 1885, 1886, 1919, 1996, 1997, 1998, 1999, 2000, 2001, 2099, 2100, 2101, 2130, 2188, 2204, 2205, 2206, 2227, 2244, 2245, 2246, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2535, 2536, 2636, 2637, 2638, 2661, 2662, 2663, 2664, 2754, 2755, 2756, 2757, 2887, 3057, 3058, 3059, 3060, 3080, 3081, 3141, 3142, 3143, 3144, 3292, 3293, 3294, 3298, 3299, 3300, 3301 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: QtAgg\n",
      "Dropped 107 epochs: 17, 109, 264, 265, 266, 267, 347, 348, 349, 758, 855, 856, 857, 938, 939, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1209, 1210, 1211, 1245, 1246, 1247, 1248, 1261, 1262, 1263, 1264, 1265, 1689, 1732, 1758, 1798, 1808, 1809, 1885, 1886, 1919, 1996, 1997, 1998, 1999, 2000, 2001, 2099, 2100, 2101, 2130, 2188, 2204, 2205, 2206, 2227, 2244, 2245, 2246, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2535, 2536, 2636, 2637, 2638, 2661, 2662, 2663, 2664, 2754, 2755, 2756, 2757, 2887, 3057, 3058, 3059, 3060, 3080, 3081, 3141, 3142, 3143, 3144, 3292, 3293, 3294, 3298, 3299, 3300, 3301\n",
      "Bad epochs dropped based on provided indices.\n"
     ]
    }
   ],
   "source": [
    "## Marked bad channels if not marked.\n",
    "%matplotlib\n",
    "mne.viz.set_browser_backend('qt') \n",
    "\n",
    "bad_trials = True \n",
    "if bad_trials == True:\n",
    "    \n",
    "    processor.drop_bad_epochs(bad_epoch_indices=bad_epoch_indices)\n",
    "\n",
    "   \n",
    "    \n",
    "if bad_trials == False:\n",
    "   \n",
    "    # plot epochs \n",
    "    processor.plotsignal(64,1,64)\n",
    "# drop bad trials \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_epochs(sub,data_dir,epochs):\n",
    "    # saves Epoch\n",
    "    epochs.save(data_dir + sub + '-epo.fif', fmt='single', overwrite=True)\n",
    "    \n",
    "    \n",
    "def save_ica(subject,directionary,ica):\n",
    "        \n",
    "        ica.save(directionary + subject+ '-ica.fif', \n",
    "        overwrite=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ica\n",
    "def get_clean(epochs, channel_list):\n",
    "    epochs.info[\"bads\"].extend(channel_list)\n",
    "\n",
    "    #ep_ds.set_montage('standard_1020')\n",
    "\n",
    "    epochs.set_montage('standard_1020')\n",
    "\n",
    "    # ep_interp = ep_ds.interpolate_bads(reset_bads=False, method='MNE', verbose=True)\n",
    "    epochs.interpolate_bads(reset_bads=False, method='spline', verbose=None)   \n",
    "    # average data \n",
    "    # use the average of all channels as reference\n",
    "    clean_epochs, ref_data = mne.set_eeg_reference(epochs, ref_channels=\"average\", projection=False)\n",
    "    del epochs\n",
    "    return clean_epochs\n",
    "\n",
    "epochs = get_clean(epochs,['T7','O1','Fpz','Fp2','TP8','A2','AF8'])\n",
    "save_epochs(participant_id,'C:/Users/s184075/Thesis/Data/Clean_epochs/',epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 64 channels (please be patient, this may take a while)\n",
      "Selecting by number: 30 components\n",
      "Fitting ICA took 125.4s.\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Automatically found muscle artifact ICA components: [6, 12, 15, 16, 18, 19, 20, 23, 26, 28]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>fastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit parameters</th>\n",
       "        <td>algorithm=parallel<br />fun=logcosh<br />fun_args=None<br />max_iter=1000<br /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>40 iterations on epochs (3553280 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | epochs decomposition, method: fastica (fit in 40 iterations on 3553280 samples), 30 ICA components (64 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mne.viz.set_browser_backend('qt') \n",
    "montage = mne.channels.make_standard_montage(\"standard_1005\") \n",
    "ep = processor.epochs\n",
    "#direc = \"C:/Users/Mabel Ife/OneDrive - Danmarks Tekniske Universitet/Dokumenter/Thesis Code/Data/epochs/\" \n",
    "#save_epochs(sub=participant_id,data_dir=direc,epochs=ep)\n",
    "direc = \"C:/Users/s184075/Thesis/epochs/\"\n",
    "save_epochs(participant_id,direc,ep)\n",
    "ep.set_montage(montage)\n",
    "#processor.fit_ica(p_id=participant_id)\n",
    "processor.fit_ica(p_id=participant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "2776 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Writing ICA solution to C:\\Users\\s184075\\Thesis\\ICA\\subj_21-ica.fif...\n"
     ]
    }
   ],
   "source": [
    "#del ep, bad_epoch_indices, Subject, file_name\n",
    "direc = \"C:/Users/s184075/Thesis/ICA/\"\n",
    "IC = processor.ICA\n",
    "IC.plot_properties(ep, picks=[6, 12, 15, 16, 18, 19, 20, 23, 26, 28])\n",
    "save_ica(participant_id,direc,IC)\n",
    "del IC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marked ICA COMPONENTS\n",
    "\n",
    "- Subject_1:  ICA000, ICA002,ICA004, ICA017, ICA021, ICA026, ICA027, ICA029.\n",
    "- Subject_2:  ICA000, ICA001, ICA009, ICA015, ICA017, ICA018, ICA020, ICA021, ICA022, ICA023, ICA025, ICA029.\n",
    "- Subject_3:  ICA000, ICA004, ICA007, ICA015, ICA016, ICA017, ICA018, ICA019 ICA022,ICA23, ICA029. \n",
    "- Subject_4:  ICA000, ICA002, ICA011, ICA015, ICA019, ICA020, ICA022, ICA026, ICA029\n",
    "- Subject_5:  ICA003, ICA004, ICA005, ICA006, ICA008, ICA011, ICA012, ICA014, ICA015, ICA016, ICA018, ICA019, ICA020, ICA021, ICA022, ICA025, ICA027, ICA028\n",
    "- Subject_7:  ICA000, ICA003, ICA008, ICA013, ICA016, ICA017, ICA018,  ICA020, ICA021, ICA022, ICA023, ICA024, ICA026, ICA027, ICA028\n",
    "- Subject_8:  ICA000, ICA002, ICA007, ICA011, ICA012, ICA021, ICa023, ICA024, ICA026, ICA027, ICA028, ICA029\n",
    "- Subject_10: ICA001, ICA002, ICA013, ICA014, ICA023, ICA025, ICA026, ICA028, ICA029\n",
    "- Subject_11: ICA001, ICA003, ICA005, ICA011, ICA012, ICA013, ICA015,  ICA016, ICA020, ICA021, ICA022, ICA023, ICA027\n",
    "- Subject_12: ICA000, ICA003, ICA011, ICA015, ICA025, ICA028, ICA029, \n",
    "- Subject_13: ICA002, ICA015, ICA020, ICA022, ICA023, ICA024, ICA025, ICA026, ICA028, ICA029.\n",
    "- Subject_14: ICA000, ICA002, ICA010, ICA011, ICA012, ICA013, ICA015, ICA017, ICA018, ICA019, ICA022, ICA023, ICA025, ICA026, ICA027, ICA028\n",
    "- Subject_15: ICA000, ICA004, ICA011, ICA013, ICA014, ICA015, ICA016, ICA017, ICA018, ICA020, ICA021, ICA022, ICA024, ICA025, ICA027, ICA029.\n",
    "- Subject_16: ICA000, ICA001, ICA003, ICA006, ICA009, ICA010, ICA012, ICA013, ICA014, ICA016, ICA017, ICA018, ICA022, ICA023, ICA024, ICA025, ICA026, ICA027, ICA028, ICA029.\n",
    "- Subject_17: ICA004, ICA015, ICA019, ICA027.\n",
    "- Subject_18: ICA000, ICA001, ICA009, ICA010, ICA012, ICA013, ICA014, ICA015, ICA017, ICA019, ICA020, ICA021, ICA022, ICA024, ICA026, ICA029.\n",
    "- Subject_20: ICA000, ICA005, ICA009, ICA010, ICA011, ICA013, ICA014, ICA015, ICA016, ICA017, ICA018, ICA019, ICA020, ICA021, ICA022, ICA024, ICA025, ICA026, ICA027.\n",
    "- Subject_21: ICA000, ICA007, ICA016, ICA018, ICA019, ICA020, ICA023, ICA026, ICA028.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying ICA to Epochs instance\n",
      "    Transforming to ICA space (30 components)\n",
      "    Zeroing out 9 ICA components\n",
      "    Projecting back using 64 PCA components\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processor.reject_artefact([0,7, 16, 18, 19, 20, 23, 26, 28])\n",
    "processor.plotsignal(64,1,64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting channel interpolation method to {'eeg': 'spline'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 95.6 mm\n",
      "Computing interpolation matrix from 63 sensor positions\n",
      "Interpolating 1 sensors\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n"
     ]
    }
   ],
   "source": [
    "processor.drop_bad_interp(['AF4'])\n",
    "clean_ep = processor.clean_epochs\n",
    "\n",
    "processor.plotsignal(64,1,64)\n",
    "direc = \"C:/Users/s184075/Thesis/clean_epochs/\" \n",
    "save_epochs(participant_id,direc,clean_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_analysis(epochs):\n",
    "        \n",
    "    # Perform wavelet analysis\n",
    "        # Perform wavelet analysis\n",
    "    freqs = np.arange(1, 31)  # interested in frequencies 1 to 30 Hz\n",
    "    n_cycles = 3 + 0.125 * freqs  # Number of cycles\n",
    "    \n",
    "    # Get the list of channel names in your EpochsArray\n",
    "    channel_names = epochs.info['ch_names']\n",
    "\n",
    "    # Create an empty list to store wavelet analyses for each channel\n",
    "    wavelet_results = []\n",
    "    # Iterate over each channel index\n",
    "    for i, channel_name in enumerate(channel_names):\n",
    "        # Skip stimulus channels\n",
    "        if channel_name.startswith('S'):\n",
    "            continue\n",
    "        \n",
    "        #clean_ep._data[:, i, :]\n",
    "        \n",
    "\n",
    "        p = mne.time_frequency.tfr_array_morlet(clean_ep._data[:, i, :], sfreq=256, freqs=freqs, n_cycles=n_cycles, use_fft=False,\n",
    "                                        decim=1, n_jobs=None, zero_mean=True, output='power')\n",
    "        \n",
    "        wavelet_results.append(p)\n",
    "        \n",
    "    combined_epochs = wavelet_results[0].copy()\n",
    "    \n",
    "    for i in range(1, len(wavelet_results)):\n",
    "        \n",
    "        combined_epochs._data += wavelet_results[i]._data\n",
    "        \n",
    "    \n",
    "    del epochs\n",
    "        \n",
    "    return combined_epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "def wave_analysis1(epochs,chunk_size = 320):\n",
    "    # Define frequency range and number of cycles\n",
    "    freqs = np.arange(1, 31)  # interested in frequencies 1 to 30 Hz\n",
    "    n_cycles = 3 + 0.125 * freqs  # Number of cycles\n",
    "\n",
    "\n",
    "    # Initialize an empty list to store wavelet analyses for each channel\n",
    "    wavelet_results = []\n",
    " # Process data in smaller time chunks\n",
    "    for i in range(0, len(epochs.times), chunk_size):\n",
    "        print(len(epochs.times))\n",
    "        start_idx = int(i)\n",
    "        end_idx = int(min(i + chunk_size, len(epochs.times)) ) # Ensure end index doesn't exceed the length of the time axis\n",
    "        chunk_data = epochs[:, :, start_idx:end_idx]\n",
    "\n",
    "        # Perform wavelet analysis using tfr_array_morlet for the current chunk\n",
    "        p_chunk = mne.time_frequency.tfr_array_morlet(chunk_data, epochs.info['sfreq'], freqs, n_cycles,\n",
    "                                                        zero_mean=True, use_fft=False, decim=1, output='power')\n",
    "\n",
    "        # Append the result for the current chunk to the list\n",
    "        wavelet_results.append(p_chunk)\n",
    "\n",
    "   \n",
    "           \n",
    "\n",
    "        # Combine the wavelet results for all chunks into one Epochs object\n",
    "    combined_epochs = wavelet_results[0].copy()\n",
    "    \n",
    "    for i in range(1, len(wavelet_results)):\n",
    "        combined_epochs._data += wavelet_results[i]._data\n",
    "\n",
    "    return combined_epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an instance of SubjectData\n",
    "subject_data_manager = SubjectData()\n",
    "\n",
    "# Add initial data for a subject\n",
    "subject_data_manager.add_subject_data('subject_001', Rereferenced_Epochs=10.0)\n",
    "\n",
    "# Later, when more data is available for the same subject\n",
    "subject_data_manager.add_subject_data('subject_001', Alpha_Left_ERD=[1, 2, 3, 4])\n",
    "\n",
    "# Save the data to a file\n",
    "subject_data_manager.save_to_file('progress.csv')\n",
    "\n",
    "# Later, when you want to continue:\n",
    "# Load the data from the file\n",
    "subject_data_manager.load_from_file('progress.csv')\n",
    "\n",
    "# Add more data for the same subject\n",
    "subject_data_manager.add_subject_data('subject_001', Beta_Left_ERD=[5, 6, 7, 8])\n",
    "\n",
    "# Save the updated data to the file again\n",
    "subject_data_manager.save_to_file('progress.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the column names and data types\n",
    "columns = {\n",
    "    'P_id': object,\n",
    "    'Rereferenced_Epochs': float,\n",
    "    'ICA': object,\n",
    "    'Alpha_Left_ERD': object,\n",
    "    'Alpha_Right_ERD': object,\n",
    "    'Beta_Left_ERD': object,\n",
    "    'Beta_Right_ERD': object,\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame with the defined columns\n",
    "df = pd.DataFrame(columns=columns.keys())\n",
    "\n",
    "# Now you have an empty DataFrame with the desired structure\n",
    "# You can add rows to it as you preprocess each subject\n",
    "ICA = DP.ICA\n",
    "# Example of adding data for a subject\n",
    "subject_data = {\n",
    "    'P_id': participant_id,\n",
    "    'Rereferenced_Epochs': rereferenced_epochs,\n",
    "    'ICA': ICA,  # Placeholder for now\n",
    "    'Alpha_Left_ERD': None,  # Placeholder for now\n",
    "    'Alpha_Right_ERD': None,  # Placeholder for now\n",
    "    'Beta_Left_ERD': None,  # Placeholder for now\n",
    "    'Beta_Right_ERD': None,  # Placeholder for now\n",
    "}\n",
    "\n",
    "# Append the data for the subject to the DataFrame\n",
    "df = pd.concat([df, pd.DataFrame([subject_data])], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Power.pickle'\n",
    "# Open the file in binary mode\n",
    "with open(file_path, 'wb') as file:\n",
    "    # Serialize and write the variable to the file\n",
    "    pickle.dump(after_wave, file)\n",
    "\n",
    "ERDS_all_new = dp.get_Erdslist(after_wave)\n",
    "dp.plotERDS_bokeh(ERDS_all_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save in dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, when more data is available for the same subjec\n",
    "ica = DP.ICA\n",
    "subject_manager.add_subject_data(participant_id,ICA=ica)\n",
    "subject_manager.add_subject_data(participant_id, Alpha_Left_ERD=[ERD_list[0][0].data[1,0,:], ERD_list[1][0].data[1,0,:], ERD_list[2][0].data[1,0,:], ERD_list[3][0].data[1,0,:]])\n",
    "subject_manager.add_subject_data(participant_id, Alpha_Right_ERD=[ERD_list[0][0].data[0,0,:], ERD_list[1][0].data[0,0,:], ERD_list[2][0].data[0,0,:], ERD_list[3][0].data[0,0,:]])\n",
    "subject_manager.add_subject_data(participant_id, Beta_Left_ERD=[ERD_list[0][1].data[1,0,:], ERD_list[1][1].data[1,0,:], ERD_list[2][1].data[1,0,:], ERD_list[3][1].data[1,0,:]])\n",
    "subject_manager.add_subject_data(participant_id, Beta_Right_ERD=[ERD_list[0][1].data[0,0,:], ERD_list[1][1].data[0,0,:], ERD_list[2][1].data[0,0,:], ERD_list[3][1].data[0,0,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the cas of no ICA components \n",
    "ica = DP.ICA\n",
    "subject_manager.add_subject_data(participant_id,ICA=ica)\n",
    "subject_manager.add_subject_data(participant_id, Alpha_Left_ERD=None)\n",
    "subject_manager.add_subject_data(participant_id, Alpha_Right_ERD=None)\n",
    "subject_manager.add_subject_data(participant_id, Beta_Left_ERD=None)\n",
    "subject_manager.add_subject_data(participant_id, Beta_Right_ERD=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
